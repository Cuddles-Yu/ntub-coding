### åŒ¯å…¥æ¨¡çµ„ ###
from datetime import datetime

from åœ°åœ–è³‡è¨Šçˆ¬èŸ².crawler.module.functions.common import *
from åœ°åœ–è³‡è¨Šçˆ¬èŸ².crawler.module.functions.EdgeDriver import EdgeDriver
from åœ°åœ–è³‡è¨Šçˆ¬èŸ².crawler.module.functions.SqlDatabase import SqlDatabase
from åœ°åœ–è³‡è¨Šçˆ¬èŸ².crawler.tables.base import *
from åœ°åœ–è³‡è¨Šçˆ¬èŸ².crawler.tables import Store, Comment, Keyword, Location, Rate, Service, Tag, OpenHour

from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By

### åˆå§‹åŒ– ###
database = SqlDatabase('mapdb', 'root', '11236018')
driver = EdgeDriver(database, url='https://www.google.com.tw/maps/preview')
urls = []

### ä¸»ç¨‹å¼ ###
if OPEN_DATA:
    match OPEN_DATA:
        case 'ç’°ä¿':
            NAME_KEY = 'é¤å»³åç¨±'
            ADDRESS_KEY = 'é¤å»³åœ°å€'
            API_PATH = 'https://data.taipei/api/v1/dataset/d706f428-b2c7-4591-9ebf-9f5cd7408f47?scope=resourceAquire&limit=1000'
        case 'å®¢å®¶':
            NAME_KEY = 'å®¢å®¶ç¾é£Ÿé¤å»³åº—å'
            ADDRESS_KEY = 'åœ°å€'
            API_PATH = 'https://data.taipei/api/v1/dataset/2f2b039d-6bff-4663-a4e5-3bd6cc98ee48?scope=resourceAquire&limit=1000'
    api_data = get_json_from_api(API_PATH)
    if api_data:
        names = [item.get(NAME_KEY) for item in api_data.get('result').get('results')]
        addresses = [item.get(ADDRESS_KEY) for item in api_data.get('result').get('results')]
        count = len(names)
        print(f'\ræ­£åœ¨çˆ¬å–APIæ‰€æœ‰å•†å®¶è³‡æ–™(å…±{count}å€‹)...\n')
        for i, (name, address) in enumerate(zip(names, addresses)):
            links, store_names = driver.search_and_scroll(name, return_one=True, show_hint=False)
            if links:
                urls.extend(links)
                print(f'\rã€ğŸ’›å·²å–å¾—ã€‘{str(i + 1).zfill(len(str(count)))}/{count} | {name} | {address}\n', end='')
            else:
                print(f'\rã€ğŸ¤æœªå–å¾—ã€‘{str(i + 1).zfill(len(str(count)))}/{count} | {name} | {address}\n', end='')
else:
    if STORES_URLS:
        urls = STORES_URLS
    else:
        if CONTINUE_CRAWLER:
            urls = database.get_urls_from_incomplete_store()
            if CONTINUE_COUNT > 0: urls = limit_list(urls, CONTINUE_COUNT)
            if not urls:
                print(f'æŸ¥ç„¡éœ€è³‡æ–™ä¿®å¾©ä¹‹å•†å®¶ï¼Œç¨‹å¼å°‡è‡ªå‹•çµæŸ...')
                driver.exit()
            print(f'è³‡æ–™å®Œæ•´æ€§ä¿®å¾©æ¨¡å¼ -> å·²é–‹å•Ÿ')

print(f'è³‡æ–™å°‡å„²å­˜è‡³è³‡æ–™åº« -> {database.name}')
if FORCE_CRAWLER: print(f'å¼·åˆ¶çˆ¬èŸ²æ¨¡å¼(è‡ªå‹•é‡è¨­å·²å­˜åœ¨å•†å®¶) -> å·²é–‹å•Ÿ')

### æŸ¥è©¢é—œéµå­—å¾Œå„²å­˜æŸ¥è©¢çµæœ ###
if not urls:
    print(f'æ­£åœ¨æœå°‹é—œéµå­— -> {SEARCH_KEYWORD}\n')
    urls, store_names = driver.search_and_save_results(SEARCH_KEYWORD)

### ä¸»çˆ¬èŸ² ###
url_count = len(urls)
if urls: urls = to_map_url(urls)
if SHUFFLE_URLS: shuffle(urls)

print(urls)

print(f'\ræ­£åœ¨æº–å‚™çˆ¬å–æ‰€æœ‰å•†å®¶é€£çµè³‡æ–™(å…±{url_count}å€‹)...\n')
for i in range(url_count):
    CRAWLER_START_TIME = datetime.now()
    # ç€è¦½å™¨è¼‰å…¥æŒ‡å®šçš„å•†å®¶åœ°åœ–é€£çµ
    driver.get(urls[i])
    # ç›´åˆ°å•†å®¶åç¨±é¡¯ç¤º(ç„¡æœ€å¤§ç­‰å€™æ™‚é–“)
    while True:
        # åœ°é»åç¨±
        title = driver.wait_for_text(By.CLASS_NAME, 'DUwDvf')
        if title.strip() != '': break
        time.sleep(0.1)
    store_item = Store.newObject(title, urls[i], mark=OPEN_DATA)
    store_item._crawler_state = 'åŸºæœ¬'

    ### æª¢æŸ¥è³‡æ–™åº«ä¸­æ˜¯å¦å·²ç¶“å­˜åœ¨æŒ‡å®šçš„å•†å®¶ ###
    if store_item.exists(database):
        crawler_state, crawler_description = store_item.get_state(database)
        if crawler_description is None or FORCE_CRAWLER:
            # 'å»ºç«‹' | 'åŸºæœ¬'
            print(f'\ræ­£åœ¨æº–å‚™é‡æ–°çˆ¬å–è³‡æ–™...', end='')
            store_item.reset(database)
        else:
            # 'æˆåŠŸ' | 'æŠ½æ¨£' | 'è¶…æ™‚' | 'ç‰¹æ®Š'
            print(f'\rã€â­å·²å­˜åœ¨ã€‘{str(i + 1).zfill(len(str(url_count)))}/{url_count} | {title} ({crawler_state})\n', end='')
            if OPEN_DATA: store_item.change_mark(database, OPEN_DATA)
            continue
        # print(f'\rã€ğŸŒåƒç…§é»ã€‘{str(i + 1).zfill(len(str(max_count)))}/{max_count} | {title}\n', end='')

    # è®€å–æ¨™ç±¤æŒ‰éˆ•
    tabs_elem, tabs_name = driver.get_tabs()
    if tabs_elem is None:
        print(f'\rã€ğŸˆšç„¡é ç±¤ã€‘{str(i + 1).zfill(len(str(url_count)))}/{url_count} | {title}\n', end='')
        store_item.change_crawler_state(database, 'é ç±¤', 'ä¸åŒ…å«è³‡è¨Šé ç±¤')
        continue
    else:
        if not set(SWITCH_TABS).issubset(set(tabs_name)):
            print(f'\rã€ğŸ†–ç¼ºé ç±¤ã€‘{str(i + 1).zfill(len(str(url_count)))}/{url_count} | {title}\n', end='')
            store_item.change_crawler_state(database, 'é ç±¤', f'ç¼ºå°‘{''.join(set(SWITCH_TABS) - set(tabs_name))}é ç±¤')
            continue

    # ç¢ºèªæ˜¯å¦ç‚ºç‰¹æ®Šå•†å®¶
    if 'åƒ¹æ ¼' in tabs_name:
        print(f'\rã€ğŸ’ç‰¹æ®Šæ€§ã€‘{str(i + 1).zfill(len(str(url_count)))}/{url_count} | {title}\n', end='')
        store_item.change_crawler_state(database, 'ç‰¹æ®Š', 'çµåˆé£¯åº—ä½å®¿æœå‹™')
        continue
    else:
        store_item._crawler_state = 'åŸºæœ¬'

    ### å•†å®¶æ¬„ä½è³‡æ–™ ###
    store_state = driver.find_elements(By.CLASS_NAME, 'fCEvvc')
    if len(store_state) > 0:
        store_item.change_tag(database, store_state[0].text)
    else:
        information_bar = driver.wait_for_element(By.CLASS_NAME, 'tAiQdd')
        if information_bar:
            store_tag = information_bar.find_elements(By.CLASS_NAME, 'DkEaL')
            if not store_tag: store_tag = information_bar.find_elements(By.CLASS_NAME, 'mgr77e')
            store_item.change_tag(database, store_tag[0].text if store_tag else None)

    # å¯èƒ½ç‚ºæ°¸ä¹…æ­‡æ¥­/æš«æ™‚é—œé–‰
    if store_item.tag:
        if any(pass_tag in store_item.tag for pass_tag in PASS_TAGS):
            print(f'\rã€â›”ä¼‘æ¥­ä¸­ã€‘{str(i + 1).zfill(len(str(url_count)))}/{url_count} | {title}\n', end='')
            store_item.change_crawler_state(database, 'ä¼‘æ¥­', 'å•†å®¶å·²æ°¸ä¹…åœæ¥­')
            continue
        elif any(remove_tag in store_item.tag for remove_tag in REMOVE_TAGS):
            print(f'\rã€ğŸ¤¡é¡åˆ¥å¤–ã€‘{str(i + 1).zfill(len(str(url_count)))}/{url_count} | {title}\n', end='')
            store_item.change_crawler_state(database, 'ç„¡æ•ˆ', 'ä¸å±¬æ–¼é¤å»³é¡åˆ¥')
            continue

    ### ç‡Ÿæ¥­è³‡è¨Šæ¨™ç±¤ ###
    print('\ræ­£åœ¨å–å¾—ç‡Ÿæ¥­è³‡è¨Š...', end='')
    # æª¢æŸ¥æ¨™ç±¤ç‹€æ…‹
    filtered_tags = [
        t for t in driver.find_elements(By.CLASS_NAME, 'RcCsl')
        if t.find_elements(By.CLASS_NAME, 'HlvSq')  # æ–°ç‰ˆæ¨™ç±¤
    ]
    if filtered_tags: filtered_tags[0].click()
    # å–å¾—ç‡Ÿæ¥­è³‡è¨Š
    open_hours_tag = driver.wait_for_element(By.CLASS_NAME, 'OqCZI')
    open_hours_dict = {}
    if open_hours_tag:
        time.sleep(0.2)
        # æ›´æ–°æ™‚é–“
        if to_bool(open_hours_tag.find_element(By.CLASS_NAME, 'OMl5r').get_attribute('aria-expanded')): open_hours_tag.click()  # (ç¢ºä¿æ¨™ç±¤é—œé–‰ä»¥å–å¾—æ›´æ–°æ™‚é–“)
        update_info = driver.find_elements(By.CLASS_NAME, 'zaf2le')
        if update_info:
            last_update_info = re.search(r'(?P<time>\d+\s*\D+å‰)', update_info[0].text.strip())
            store_item._last_update = last_update_info.group() if last_update_info else None
        time.sleep(0.2)
        # ç‡Ÿæ¥­æ™‚é–“
        if not to_bool(open_hours_tag.find_element(By.CLASS_NAME, 'OMl5r').get_attribute('aria-expanded')): open_hours_tag.click()  # (æ²’æ‰“é–‹æ¨™ç±¤æœƒæŠ“ä¸åˆ°å…ƒç´ æ–‡å­—)
        days_of_week = driver.find_element(By.CLASS_NAME, 't39EBf').find_elements(By.CLASS_NAME, 'y0skZc')
        for day in days_of_week:
            # æ˜ŸæœŸ
            day_of_week = day.find_element(By.CLASS_NAME, 'ylH6lf').find_element(By.TAG_NAME, 'div').text
            open_hour_info = day.find_elements(By.CLASS_NAME, 'G8aQO')
            # æ™‚é–“
            open_hours_list = [
                {'open': time.text.split('â€“')[0], 'close': time.text.split('â€“')[1]}
                for time in open_hour_info if ':' in time.text
            ]
            if not open_hours_list and open_hour_info:
                if '24 å°æ™‚ç‡Ÿæ¥­' in open_hour_info[0].text: open_hours_list.append({'open': '00:00', 'close': '24:00'})
            open_hours_dict[day_of_week] = open_hours_list if open_hours_list else None
    if filtered_tags: driver.wait_for_click(By.CLASS_NAME, 'hYBOP')  # è¿”å›

    # é‡æ–°è®€å–æ¨™ç±¤æŒ‰éˆ•
    tabs_elem, tabs_name = driver.get_tabs()

    # æ¨™ç±¤æŒ‰éˆ• - [ç¸½è¦½]/è©•è«–/ç°¡ä»‹
    if 'ç¸½è¦½' in tabs_name: driver.click_element(tabs_elem[tabs_name.index('ç¸½è¦½')])

    rate_item = Rate.newObject()
    location_item = Location.newObject()

    ### å–å¾—æ¨™ç±¤è³‡è¨Š ###
    print('\ræ­£åœ¨å–å¾—åœ°é»è³‡è¨Š...', end='')
    labels = {
        'åœ°å€': None,
        'ç¶²ç«™': None,
        'é›»è©±è™Ÿç¢¼': None,
        'Plus Code': None
    }
    tags = driver.find_elements(By.CLASS_NAME, 'RcCsl')
    for tag in tags:
        items = tag.find_elements(By.CLASS_NAME, 'CsEnBe')
        if not items: continue
        label = items[0].get_attribute('aria-label')
        href = items[0].get_attribute('href')
        if label and ': ' in label:
            name, value = label.strip().split(': ', 1)
            labels[name] = href if href else value

    store_item._website = labels['ç¶²ç«™']
    if labels['é›»è©±è™Ÿç¢¼']: store_item._phone_number = labels['é›»è©±è™Ÿç¢¼'].replace(' ', '-')

    # åœ°é»æ¬„ä½è³‡æ–™
    if labels['Plus Code']:
        village, city, district = get_split_from_plus_code(labels['Plus Code'])
        location_item.vil = village if village else None
        location_item.city = city
        location_item.dist = district

    if labels['åœ°å€']:
        postal, city, district, detail = get_split_from_address(labels['åœ°å€'])
        if (location_item.city and not city) or (location_item.dist and not district):
            simple_address = labels['åœ°å€']
            if location_item.get_city(): simple_address = simple_address.replace(location_item.get_city(), '')
            if location_item.get_dist(): simple_address = simple_address.replace(location_item.get_dist(), '')
            postal, detail = get_split_from_simple_address(simple_address)

        location_item.postal_code = postal if postal else None
        if not location_item.city: location_item.city = city if city else None
        if not location_item.dist: location_item.dist = district if district else None
        location_item.details = detail if detail else None

    if not (location_item.get_city() and location_item.get_dist() and location_item.get_details()):
        print(f'\rã€ğŸ—ºï¸ç„¡å®šä½ã€‘{str(i + 1).zfill(len(str(url_count)))}/{url_count} | {title}\n', end='')
        store_item.change_crawler_state(database, 'å®šä½', 'ç„¡æ³•è§£æåœ°å€çµæ§‹')
        continue

    if TARGET_CITIES and location_item.get_city() not in TARGET_CITIES:
        print(f'\rã€ğŸŒç¯„åœå¤–ã€‘{str(i + 1).zfill(len(str(url_count)))}/{url_count} | {title}\n', end='')
        store_item.change_crawler_state(database, 'è¶Šç•Œ', 'ä¸æ”¯æ´éé›™åŒ—åœ°å€')
        continue

    ### å•†å®¶ç›¸ç‰‡ ###
    print('\ræ­£åœ¨å–å¾—å•†å®¶ç›¸ç‰‡...', end='')
    store_img = driver.wait_for_element(By.CLASS_NAME, 'aoRNLd')
    time.sleep(0.5)
    if store_img:
        image_url = store_img.find_element(By.TAG_NAME, 'img').get_attribute('src')
        store_item._preview_image = image_url
        store_item._image = download_image_as_binary(image_url)

    ### æœå‹™é …ç›® ###
    print('\ræ­£åœ¨å–å¾—æœå‹™é …ç›®...', end='')
    service_dict = {}
    if 'ç°¡ä»‹' in tabs_name:
        # æ¨™ç±¤æŒ‰éˆ• - ç¸½è¦½/è©•è«–/[ç°¡ä»‹]
        tabs_elem[tabs_name.index('ç°¡ä»‹')].click()
        time.sleep(0.2)
        # å•†å®¶ç°¡ä»‹ (å·²åœç”¨)
        # description = driver.wait_for_element(By.CLASS_NAME, 'PbZDve')
        # if description: store_item._description = description.find_element(By.CLASS_NAME, 'ZqFyf').find_element(By.TAG_NAME, 'span').text
        # æœå‹™é¡åˆ¥
        for category in driver.find_elements(By.CLASS_NAME, 'iP2t7d'):
            category_name = category.find_element(By.CLASS_NAME, 'iL3Qke').text
            # æœå‹™é …ç›®
            for service in category.find_elements(By.CLASS_NAME, 'WeoVJe'):  # æ²’æœ‰æä¾›çš„æœå‹™
                service_dict[service.find_element(By.TAG_NAME, 'span').text] = (category_name, 0)
            for service in category.find_elements(By.CLASS_NAME, 'hpLkke'):  # æ‰€æœ‰æä¾›çš„æœå‹™
                if service_dict.get(service.find_element(By.TAG_NAME, 'span').text) is None: service_dict[service.find_element(By.TAG_NAME, 'span').text] = (category_name, 1)

    ### å„²å­˜è‡³'é—œéµå­—'è³‡æ–™è¡¨ [å·²åœç”¨]
    # if store_item.tag:
    #     tag_item = Tag.Tag(
    #         tag=store_item.tag,
    #         category=None
    #     ).insert_if_not_exists(database)

    ### å„²å­˜å•†å®¶è³‡æ–™ï¼Œä¸¦å–å¾—å…¶ store_id ###
    try:
        store_item.update_if_exists(database)
    except Exception as e:
        print(f'\rã€âŒå·²å¤±æ•—ã€‘{str(i + 1).zfill(len(str(url_count)))}/{url_count} | {e} - {title}\n', end='')
        continue
    store_id = store_item.get_id(database)
    rate_item.store_id = store_id
    location_item._store_id = store_id

    # æ¨™ç±¤æŒ‰éˆ• - ç¸½è¦½/[è©•è«–]/ç°¡ä»‹
    print('\ræ­£åœ¨å–å¾—å•†å®¶è©•è«–...', end='')
    if 'è©•è«–' in tabs_name:
        tabs_elem[tabs_name.index('è©•è«–')].click()
        # å–å¾—è©•è«–æ˜Ÿç´š
        rating = driver.wait_for_element(By.CLASS_NAME, 'jANrlb')
        if rating:
            rate_item.avg_rating = float(rating.find_element(By.CLASS_NAME, 'fontDisplayLarge').text)
            rate_item.total_reviews = int(''.join(re.findall(r'\d+', rating.find_element(By.CLASS_NAME, 'fontBodySmall').text)))

    if rate_item.total_reviews == 0:
        print(f'\rã€ğŸ“ç„¡è©•è«–ã€‘{str(i + 1).zfill(len(str(url_count)))}/{url_count} | {title}\n', end='')
        store_item.change_crawler_state(database, 'å¤±æ•—', 'è©•è«–ç¸½æ•¸ç‚ºé›¶')
    elif rate_item.total_reviews < MINIMUM_SAMPLES:
        print(f'\rã€âŒå·²å¤±æ•—ã€‘{str(i + 1).zfill(len(str(url_count)))}/{url_count} | è©•è«–ç¸½æ•¸ä½æ–¼æœ€ä½æ¨£æœ¬æ•¸ - {title}\n', end='')
        store_item.change_crawler_state(database, 'å¤±æ•—', 'è©•è«–ç¸½æ•¸ä¸è¶³')
        continue

    comments_dict = {}
    keywords_dict = {}
    if rate_item.total_reviews > 0:
        commentContainer = driver.wait_for_element(By.CLASS_NAME, 'dS8AEf')  # è©•è«–é¢æ¿
        keywords_dict = driver.get_keywords_dict()  # å–å¾—é—œéµå­—
        orders = {
            'æœ€ç›¸é—œ': {
                'æœ€å¤§æ¨£æœ¬': MAXIMUM_SAMPLES,
                'çµ±è¨ˆç‰©ä»¶': rate_item,
                'é¡å‹é™£åˆ—': [1, 0, 0],
                'æ¨£æœ¬å‹æ…‹': None,
                'éæ¿¾è©•è«–': 0
            },
            'è©•åˆ†æœ€é«˜': {
                'æœ€å¤§æ¨£æœ¬': REFERENCE_SAMPLES,
                'çµ±è¨ˆç‰©ä»¶': rate_item.newObject(),
                'é¡å‹é™£åˆ—': [0, 1, 0],
                'æ¨£æœ¬å‹æ…‹': None
            },
            'è©•åˆ†æœ€ä½': {
                'æœ€å¤§æ¨£æœ¬': REFERENCE_SAMPLES,
                'çµ±è¨ˆç‰©ä»¶': rate_item.newObject(),
                'é¡å‹é™£åˆ—': [0, 0, 1],
                'æ¨£æœ¬å‹æ…‹': None
            }
        }
        for order_type, settings in orders.items():
            if order_type != 'æœ€ç›¸é—œ' and (orders['æœ€ç›¸é—œ']['æ¨£æœ¬å‹æ…‹'] is None or orders['æœ€ç›¸é—œ']['çµ±è¨ˆç‰©ä»¶'].total_browses >= settings['çµ±è¨ˆç‰©ä»¶'].total_reviews): break
            ### æ»¾å‹•è©•è«–é¢æ¿å–å¾—æ‰€æœ‰è©•è«– ###
            driver.switch_to_order(order_type)
            # åˆå§‹è®Šæ•¸
            start_time = time.time()
            CHECK_INTERVAL = 5  # æ¯æ¬¡æª¢æŸ¥è©•è«–çš„é–“éš”æ¬¡æ•¸
            current_total_target = 0
            current_total_reviews_count = 0
            current_filtered_reviews_count = 0
            scroll_count = 0  # è¨˜éŒ„æ²å‹•æ¬¡æ•¸
            while True:
                driver.move_to_element(commentContainer.find_elements(By.CLASS_NAME, 'jftiEf')[-1])
                commentContainer.send_keys(Keys.PAGE_DOWN)
                total_reviews = commentContainer.find_elements(By.CLASS_NAME, 'jftiEf')
                scroll_count += 1
                time.sleep(0.5)
                # æª¢æŸ¥æ˜¯å¦å‡ºç¾æ–°çš„è©•è«–
                if scroll_count % CHECK_INTERVAL == 0:
                    scroll_count = 0
                    if current_total_reviews_count != len(total_reviews):
                        start_time = time.time()
                        filtered_reviews = [
                            c for c in total_reviews
                            if not ('å¹´' in c.find_element(By.CLASS_NAME, 'rsqaWe').text and int(re.search(r'\d+', c.find_element(By.CLASS_NAME, 'rsqaWe').text).group()) > MAX_COMMENT_YEARS)
                        ]
                        current_total_reviews_count = len(total_reviews)
                        current_filtered_reviews_count = len(filtered_reviews)

                    match order_type:
                        case 'æœ€ç›¸é—œ':
                            total_withcomments = [
                                c for c in filtered_reviews
                                if c.find_elements(By.CLASS_NAME, HAS_COMMENT_CLASS)  # æœ‰æ–‡å­—å…§å®¹
                            ]
                            current_total_target = len(total_withcomments)
                            if current_total_reviews_count >= settings['çµ±è¨ˆç‰©ä»¶'].total_reviews:
                                settings['æ¨£æœ¬å‹æ…‹'] = 'all'
                                break
                            if current_total_target >= settings['æœ€å¤§æ¨£æœ¬']:
                                settings['æ¨£æœ¬å‹æ…‹'] = 'sample'
                                break
                        case 'è©•åˆ†æœ€é«˜' | 'è©•åˆ†æœ€ä½':
                            last_score = int(total_reviews[-1].find_element(By.CLASS_NAME, 'kvMYJc').get_attribute('aria-label').split(' ')[0])
                            if order_type == 'è©•åˆ†æœ€é«˜':
                                total_highrating_withcomments = [
                                    c for c in filtered_reviews
                                    if c.find_elements(By.CLASS_NAME, HAS_COMMENT_CLASS) and int(c.find_element(By.CLASS_NAME, 'kvMYJc').get_attribute('aria-label').split(' ')[0]) >= HIGHRATING_SCORE
                                ]
                                current_total_target = len(total_highrating_withcomments)
                                if current_total_target >= settings['æœ€å¤§æ¨£æœ¬']:
                                    settings['æ¨£æœ¬å‹æ…‹'] = 'complete'
                                    break
                                if last_score < HIGHRATING_SCORE:
                                    settings['æ¨£æœ¬å‹æ…‹'] = 'all'
                                    break
                            elif order_type == 'è©•åˆ†æœ€ä½':
                                total_lowrating_withcomments = [
                                    c for c in filtered_reviews
                                    if c.find_elements(By.CLASS_NAME, HAS_COMMENT_CLASS) and int(c.find_element(By.CLASS_NAME, 'kvMYJc').get_attribute('aria-label').split(' ')[0]) <= LOWRATING_SCORE
                                ]
                                current_total_target = len(total_lowrating_withcomments)
                                if current_total_target >= settings['æœ€å¤§æ¨£æœ¬']:
                                    settings['æ¨£æœ¬å‹æ…‹'] = 'complete'
                                    break
                                if last_score > LOWRATING_SCORE:
                                    settings['æ¨£æœ¬å‹æ…‹'] = 'all'
                                    break

                    if time.time() - start_time > (MAXIMUM_TIMEOUT + (current_total_reviews_count ** 0.5) * 0.8):
                        if current_total_reviews_count >= settings['æœ€å¤§æ¨£æœ¬']:
                            settings['æ¨£æœ¬å‹æ…‹'] = 'limit'
                        else:
                            settings['æ¨£æœ¬å‹æ…‹'] = 'timeout'
                        break

                print(f'\ræ­£åœ¨å–å¾—æ‰€æœ‰{order_type}è©•è«– | é€²åº¦:{current_total_target} | éæ¿¾:{current_filtered_reviews_count}/ç€è¦½:{current_total_reviews_count}/ç¸½æ•¸:{settings['çµ±è¨ˆç‰©ä»¶'].total_reviews} | ' +
                      f'{'â–®'*(scroll_count+1)}{'â–¯'*(CHECK_INTERVAL-scroll_count-1)} | {title}...', end='')

            ### æ‹†åˆ†è©•è«–å…ƒç´  ###
            match order_type:
                case 'æœ€ç›¸é—œ':
                    settings['éæ¿¾è©•è«–'] = current_filtered_reviews_count
                    mixed_reviews = limit_list(filtered_reviews, settings['æœ€å¤§æ¨£æœ¬'])
                    total_withcomments = limit_list(total_withcomments, settings['æœ€å¤§æ¨£æœ¬'])
                    total_withoutcomments = [
                        c for c in mixed_reviews
                        if not c.find_elements(By.CLASS_NAME, HAS_COMMENT_CLASS)  # æ²’æ–‡å­—å…§å®¹
                    ]
                    # (åœ¨æ··åˆç•™è¨€è£¡é¢åŒ…å«æ–‡å­—å…§å®¹çš„æ•¸é‡)
                    mix_comments_count = len([
                        c for c in mixed_reviews
                        if c.find_elements(By.CLASS_NAME, HAS_COMMENT_CLASS)  # æ²’æ–‡å­—å…§å®¹
                    ])
                    additional_comments = limit_list([
                        c for c in exclude_list(filtered_reviews, settings['æœ€å¤§æ¨£æœ¬'])
                        if c.find_elements(By.CLASS_NAME, HAS_COMMENT_CLASS)  # æœ‰æ–‡å­—å…§å®¹
                    ], settings['æœ€å¤§æ¨£æœ¬'] - mix_comments_count)
                    total_samples = mixed_reviews + additional_comments
                    settings['çµ±è¨ˆç‰©ä»¶'].total_browses = len(total_reviews)
                    settings['çµ±è¨ˆç‰©ä»¶'].total_withcomments = current_total_target
                    settings['çµ±è¨ˆç‰©ä»¶'].total_withoutcomments = len(total_withoutcomments)
                    settings['çµ±è¨ˆç‰©ä»¶'].mixreviews_count = len(mixed_reviews)
                    settings['çµ±è¨ˆç‰©ä»¶'].additionalcomments_count = len(additional_comments)
                case 'è©•åˆ†æœ€é«˜':
                    total_samples = limit_list(total_highrating_withcomments, settings['æœ€å¤§æ¨£æœ¬'])
                case 'è©•åˆ†æœ€ä½':
                    total_samples = limit_list(total_lowrating_withcomments, settings['æœ€å¤§æ¨£æœ¬'])

            ### ç´€éŒ„çµ±è¨ˆè³‡æ–™ ###
            settings['çµ±è¨ˆç‰©ä»¶'].total_samples = len(total_samples)

            if order_type == 'æœ€ç›¸é—œ' and settings['çµ±è¨ˆç‰©ä»¶'].total_samples < MINIMUM_SAMPLES:
                print(f'\rã€âŒå·²å¤±æ•—ã€‘{str(i + 1).zfill(len(str(url_count)))}/{url_count} | è©•è«–æ¨£æœ¬å°‘æ–¼æœ€ä½éœ€æ±‚{MINIMUM_SAMPLES}å€‹ - {title}\n', end='')
                store_item.change_crawler_state(database, 'å¤±æ•—', f'æœ€ç›¸é—œè©•è«–æ¨£æœ¬ä¸è¶³')
                settings['æ¨£æœ¬å‹æ…‹'] = None
                continue

            ### æå–è©•è«–å…§å®¹ ###
            sum_score = 0
            sum_responses = 0
            for index in range(len(total_samples)):
                print(f'\ræ­£åœ¨æå–æ‰€æœ‰{order_type}è©•è«–å…§å®¹({index + 1}/{len(total_samples)})...', end='')
                data_review_id = total_samples[index].get_attribute('data-review-id')
                contents_element = total_samples[index].find_elements(By.CLASS_NAME, 'MyEned')
                score = int(total_samples[index].find_element(By.CLASS_NAME, 'kvMYJc').get_attribute('aria-label').split(' ')[0])
                sum_score += score if contents_element else 0
                sum_responses += 1 if total_samples[index].find_elements(By.CLASS_NAME, 'CDe7pd') else 0
                if data_review_id not in comments_dict:
                    # æŒ‰ä¸‹ã€Œå…¨æ–‡ã€ä»¥å±•é–‹éé•·çš„è©•è«–å…§å®¹
                    expand_comment = total_samples[index].find_elements(By.CLASS_NAME, 'w8nwRe')
                    time.sleep(0.1)
                    if expand_comment: expand_comment[0].send_keys(Keys.ENTER)
                    # å–å¾—ç•™è¨€å…§å®¹
                    contents = contents_element[0].find_element(By.CLASS_NAME, 'wiI7pd').text if contents_element else None
                    comment_time = total_samples[index].find_element(By.CLASS_NAME, 'rsqaWe').text
                    level = re.search(r'ba(?P<level>\d+)', total_samples[index].find_element(By.CLASS_NAME, 'NBa7we').get_attribute('src'))
                    # å–å¾—ä½¿ç”¨è€…ç¶“é©—
                    user_experiences_dict = {}
                    user_experiences = total_samples[index].find_elements(By.CLASS_NAME, 'RfDO5c')
                    for ue in user_experiences:
                        parent_element = driver.find_parent_element(ue, 2)
                        line = parent_element.find_elements(By.CLASS_NAME, 'RfDO5c')
                        if len(line) == 1:
                            span = line[0].find_element(By.TAG_NAME, 'span').text.split('ï¼š')
                            if span[0] in EXPERIENCE_TARGET and span[1]:
                                numbers = re.search(r'\d+', span[1])
                                if numbers: user_experiences_dict[span[0]] = int(numbers.group())
                        else:
                            dishes = []
                            experience = []
                            for review_tag in line:
                                experience.append(review_tag.find_element(By.TAG_NAME, 'span').text)
                            if experience[0] in RECOMMEND_DISHES:
                                if experience[1]: dishes = keyword_separator(experience[1])
                                if dishes:
                                    for dish in dishes:
                                        if not keyword_filter(dish): continue
                                        keywords_dict[dish] = (keywords_dict.get(dish)[0] + 1, 'recommend') if keywords_dict.get(dish) else (1, 'recommend')

                # å„²å­˜è©•è«–ç‰©ä»¶
                if comments_dict.get(data_review_id) is None:
                    comments_dict[data_review_id] = {
                        'id': len(comments_dict) + 1,
                        'contents': contents,
                        'time': comment_time,
                        'rating': score,
                        'has_image': 1 if total_samples[index].find_elements(By.CLASS_NAME, 'KtCyie') else 0,
                        'food_rating': user_experiences_dict.get(EXPERIENCE_TARGET[0]) if user_experiences_dict else None,
                        'service_rating': user_experiences_dict.get(EXPERIENCE_TARGET[1]) if user_experiences_dict else None,
                        'atmosphere_rating': user_experiences_dict.get(EXPERIENCE_TARGET[2]) if user_experiences_dict else None,
                        'contributor_level': int(level.group('level')) + 2 if level else 0,
                        'sample_of_most_relevant': settings['é¡å‹é™£åˆ—'][0],
                        'sample_of_highest_rating': settings['é¡å‹é™£åˆ—'][1],
                        'sample_of_lowest_rating': settings['é¡å‹é™£åˆ—'][2]
                    }
                else:
                    if settings['é¡å‹é™£åˆ—'][0] == 1:
                        comments_dict[data_review_id]['sample_of_most_relevant'] = 1
                    elif settings['é¡å‹é™£åˆ—'][1] == 1:
                        comments_dict[data_review_id]['sample_of_highest_rating'] = 1
                    elif settings['é¡å‹é™£åˆ—'][2] == 1:
                        comments_dict[data_review_id]['sample_of_lowest_rating'] = 1

            settings['çµ±è¨ˆç‰©ä»¶'].store_responses = sum_responses
            if settings['çµ±è¨ˆç‰©ä»¶'].total_withcomments > 0: settings['çµ±è¨ˆç‰©ä»¶'].real_rating = round(sum_score / settings['çµ±è¨ˆç‰©ä»¶'].total_withcomments, 1)

    # ç­‰å¾…ç¶²å€åˆ—é¡¯ç¤ºåº§æ¨™ä½ç½®å¾Œå–å¾—åº§æ¨™ä½ç½®
    print('\ræ­£åœ¨å–å¾—åœ°é»åº§æ¨™...', end='')
    while True:
        if '@' in driver.current_url:
            coordinate = driver.current_url.split('@')[1].split(',')[0:2]
            location_item._longitude = coordinate[1]
            location_item._latitude = coordinate[0]
            break
        time.sleep(1)

    ### å„²å­˜è‡³è³‡æ–™åº« ###
    # ç‡Ÿæ¥­æ™‚é–“
    print('\ræ­£åœ¨å„²å­˜ç‡Ÿæ¥­æ™‚é–“çµæ§‹...', end='')
    openhour_counter = 0
    for day_of_week, open_list in open_hours_dict.items():
        if open_list:
            for open_time in open_list:
                openhour_counter += 1
                openhours_item = OpenHour.OpenHour(
                    store_id=store_id,
                    sid=openhour_counter,
                    day_of_week=day_of_week,
                    open_time=open_time['open'],
                    close_time=open_time['close']
                ).insert(database)
        else:
            openhour_counter += 1
            openhours_item = OpenHour.OpenHour(
                store_id=store_id,
                sid=openhour_counter,
                day_of_week=day_of_week,
                open_time=None,
                close_time=None
            ).insert(database)
    # æœå‹™
    for index, (properties, state) in enumerate(service_dict.items()):
        print(f'\ræ­£åœ¨å„²å­˜æœå‹™é …ç›®çµæ§‹({index+1}/{len(service_dict.items())})...', end='')
        service_item = Service.Service(
            store_id=store_id,
            sid=index+1,
            properties=properties,
            category=state[0],
            state=state[1]
        ).insert(database)
    # é—œéµå­—
    for index, (word, value) in enumerate(keywords_dict.items()):
        print(f'\ræ­£åœ¨å„²å­˜é—œéµå­—çµæ§‹({index+1}/{len(keywords_dict.items())})...', end='')
        keyword_item = Keyword.Keyword(
            store_id=store_id,
            word=word,
            count=value[0],
            source=value[1],
            image_url=None,
            source_url=None
        )
        if AUTO_SEARCH_IMAGE and keyword_item.is_recommend():
            keyword_item.insert_after_search(driver, database, store_item.get_branch_title())
        else:
            keyword_item.insert_if_not_exists(database)
    # è©•è«–
    not_only_samples = orders['è©•åˆ†æœ€é«˜']['æ¨£æœ¬å‹æ…‹'] is not None
    for index, (data_id, value) in enumerate(comments_dict.items()):
        print(f'\ræ­£åœ¨å„²å­˜è©•è«–çµæ§‹({index+1}/{len(comments_dict.items())})...', end='')
        Comment.Comment(
            store_id=store_id,
            sid=value['id'],
            data_id=data_id,
            contents=value['contents'],
            time=value['time'],
            rating=value['rating'],
            has_image=value['has_image'],
            food_rating=value['food_rating'],
            service_rating=value['service_rating'],
            atmosphere_rating=value['atmosphere_rating'],
            contributor_level=value['contributor_level'],
            environment_state=None,
            price_state=None,
            product_state=None,
            service_state=None,
            sample_of_most_relevant=value['sample_of_most_relevant'] if not_only_samples else 1,
            sample_of_highest_rating=value['sample_of_highest_rating'] if not_only_samples else 1,
            sample_of_lowest_rating=value['sample_of_lowest_rating'] if not_only_samples else 1
        ).update_if_exists(database)
    # è©•åˆ†
    print('\ræ­£åœ¨å„²å­˜è©•åˆ†è³‡æ–™...', end='')
    rate_item.insert_if_not_exists(database)
    # åœ°é»
    print('\ræ­£åœ¨å„²å­˜åœ°é»è³‡æ–™...', end='')
    location_item.insert_if_not_exists(database)

    # è¨ˆç®—æ™‚é–“å·®
    TIME_DIFFERENCE = datetime.now() - CRAWLER_START_TIME
    MINUTES_ELAPSE = TIME_DIFFERENCE.total_seconds() / 60

    ### è©•ä¼°å®Œæˆç‹€æ…‹ ###
    match orders['æœ€ç›¸é—œ']['æ¨£æœ¬å‹æ…‹']:
        case 'all':
            print(f'\rã€âœ…å·²å®Œæˆã€‘{str(i + 1).zfill(len(str(url_count)))}/{url_count} | è€—æ™‚:{MINUTES_ELAPSE:.2f}åˆ†é˜ | {store_item.get_code(database)} | ç¸½æ•¸:{rate_item.total_reviews}\n', end='')
            store_item.change_crawler_state(database, 'æˆåŠŸ', 'å–å¾—å®Œæ•´è³‡æ–™')
        case 'sample':
            print(f'\rã€ğŸ“å·²æŠ½æ¨£ã€‘{str(i + 1).zfill(len(str(url_count)))}/{url_count} | è€—æ™‚:{MINUTES_ELAPSE:.2f}åˆ†é˜ | {store_item.get_code(database)} | ' +
                  f'ç›¸é—œ:{rate_item.total_samples}/æœ€é«˜:{orders['è©•åˆ†æœ€é«˜']['çµ±è¨ˆç‰©ä»¶'].total_samples}/æœ€ä½:{orders['è©•åˆ†æœ€ä½']['çµ±è¨ˆç‰©ä»¶'].total_samples} | ç¸½æ•¸:{rate_item.total_reviews})\n', end='')
            store_item.change_crawler_state(database, 'æŠ½æ¨£', 'å–å¾—æ¨£æœ¬è³‡æ–™')
        case 'limit':
            print(f'\rã€ğŸ“œå·²ä¸Šé™ã€‘{str(i + 1).zfill(len(str(url_count)))}/{url_count} | è€—æ™‚:{MINUTES_ELAPSE:.2f}åˆ†é˜ | {store_item.get_code(database)} | ' +
                  f'ç›¸é—œ:{rate_item.total_samples}/æœ€é«˜:{orders['è©•åˆ†æœ€é«˜']['çµ±è¨ˆç‰©ä»¶'].total_samples}/æœ€ä½:{orders['è©•åˆ†æœ€ä½']['çµ±è¨ˆç‰©ä»¶'].total_samples} | ' +
                  f'éæ¿¾:{orders['æœ€ç›¸é—œ']['éæ¿¾è©•è«–']}/ç€è¦½:{rate_item.total_browses}/ç¸½æ•¸:{rate_item.total_reviews}\n', end='')
            store_item.change_crawler_state(database, 'å®Œæˆ', 'å–å¾—æœ€å¤§ä¸Šé™')
        case 'timeout':
            print(f'\rã€â±ï¸å·²è¶…æ™‚ã€‘{str(i + 1).zfill(len(str(url_count)))}/{url_count} | è€—æ™‚:{MINUTES_ELAPSE:.2f}åˆ†é˜ | {store_item.get_code(database)} | ' +
                  f'ç›¸é—œ:{rate_item.total_samples}/æœ€é«˜:{orders['è©•åˆ†æœ€é«˜']['çµ±è¨ˆç‰©ä»¶'].total_samples}/æœ€ä½:{orders['è©•åˆ†æœ€ä½']['çµ±è¨ˆç‰©ä»¶'].total_samples} | ' +
                  f'éæ¿¾:{orders['æœ€ç›¸é—œ']['éæ¿¾è©•è«–']}/ç€è¦½:{rate_item.total_browses}/ç¸½æ•¸:{rate_item.total_reviews}\n', end='')
            store_item.change_crawler_state(database, 'è¶…æ™‚', 'è¶…å‡ºçˆ¬èŸ²æ™‚é–“é™åˆ¶')

    driver.refresh()

print('\rå·²å„²å­˜æ‰€æœ‰æœå°‹çµæœçš„è³‡æ–™ï¼', end='')
driver.exit()
